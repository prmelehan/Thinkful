# 1.4.3 A/A Testing

Breaking A/B Tests using A/A Tests.

## Problem #1

*You're testing advertising emails for a bathing suit company and you test one version of the email in February and the other in May.*


Assuming the following:

- 	50/50 random sample split
-  Previous customers only (as whereelse would we get the emails)
-  No filtering on location. So basically any previous customer, anywhere.
-  Sample size is large enough (all customers on a large bathing suit website)
-  Opt-In newsletter customers (people who allow for newsletters)

One immediate problem with this is that, in general, people buy bathing suits towards summer time, as most parts of the western hemisphere are freezing cold any other time of the year. Additionally, the people buying bathing suits in February are probably looking for deals, so this might not work well with tracking revenue amounts. In addition, having chosen only opt-in customers limits general reach. People who opt-in, as opposed to opting out, to a newsletter will have more of a positive relationship with the company, making it more likely that people will click the email promo.

## Problem #2

*You open a clinic to treat anxiety and find that the people who visit show a higher rate of anxiety than the general population.*

Assuming the following:

- People are walking into the clinic willingly (i.e. they aren't chosen)
- The clinic is free
- The clinic is close to public transportation

One immediate issue is pretty straightforward, the clinic specializing in anxiety. Duh. The people who go to a clinic that treats anxiety probably are feeling anxious. If the clinic were to be a paid service, it might be limiting the sample of people arriving, same if it were only reachable by car. That would weed out low income people in the observation. *Self-Selection Bias*. This could be converted into an experiment about above average axiety levels within a population of people who have axiety. So: "Our clinic found that our patients self-report their axiety much higher than the national average. We found our average score to be x standard-deviations above the national mean".


## Problem #3

*You launch a new ad billboard based campaign and see an increase in website visits in the first week*

Assuming the following:

- The billboard was put in a highly trafficked area
- The billboard linked to the website, such as "go to www.lions4sale.com"

This one is tricky, but my first inclination would be to ask: "Were you running any other ads during this time?". Was the billboard the only one? Did they do heavy social media advertising? Did they just recently increase their SEO presense?

## Problem #4

*You launch a loyalty program but see no change in visits in the first week*

Assuming the following:

- No advertising for the loyalty program was performed

This is tricky because unless the site has a **loyal** following, most new products / programs need help establishing themselves, even with former customers. If VW announces a new "free tire program", that's great, but since I don't check the VW website, like ever, how would that influence me to sign up for the program? We cannot conclude that this is a failure of the program itself for 2 main reasons:

1. No advertising has been performed
2. 1 week is hilariously short. 1-3 months seems more appropriate to measure performance of a product. Maybe even a year.