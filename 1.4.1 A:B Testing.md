# 1.4.1 A/B Testing

This document will outline the potential processes for constructing effective A/B tests for a set of example problems


## Problem #1

*Does a new supplement help people sleep better?*

First, we need to constitute what "sleep better" means. For the sake of this experiment, we will assume that we do the testing in-house.

### Versions

For control and test versions, we will use subjects that stay in the same room environment (to avoid external influences like location based noise or other influences that could skew our data).

The test should be blind (double blind or triple blind, idk), as to eliminate possible placebo. Which means we would need 4 test groups effectively. 1 group that knows they're taking the sleep drug. 1 group that is being told they're taking the sleep drug (placebo). 1 group that would be told it's a placebo when they're actually taking the drug, and the last group where they're not taking the drug at all.

### Sample

The sample should be a random sample of people. We should try to select people from different financial backgrounds, locations, etc. I say financial backgrounds because that will allow us to have a group of people that vary in their day-to-day stress levels. People who are always trying to pay bills may be more stressed, similarly, extreme high income individuals might also have a high amount of stress that could influence our test. Basically just take a random sample, and randomly split in half to form groups A & B.


### Hypothesis

My hypothesis is that the group of individuals who are taking the supplement will have a better nights sleep. (will sleep longer, and wake up less)

### Outcomes

The metric I will use to determine an effective outcome (one that meets my hypothesis) is hours slept. This has the potential of being corrupted because people sleep naturally for different periods of time. This should average out though based on the randomness of the sample.


### Other Measured Variables

The other variables I can measure could be: # times woken during sleep period, age, sex, self-reported stress level (1-5), blood pressure, weight, height. Most are all fairly un-biased measures (except for self-reported stress level). Other variables are: taking drug, not taking drug, know they're not taking drug, know they're taking drug


## Problem #2

*Will new uniforms help a gym's business?*

First, we need to pick a metric to measure the claim "help a gyms business". For this, I will choose foot traffic per month. This one will be tricky.


### Versions

We could do control months, and test months. One month without the uniform, one with. We have two groups, roughly random, catching gym rats and infrequent joggers.

### Sample

Our sample would be all people going into the gym during the control and testing periods.


### Hypothesis

My hypothesis is that uniforms will increase foot traffic to the gym.

### Outcomes
I expect that we increase foot traffic by 15%. So the month we incorporate uniforms, 15% more people will arrive at the gym. We can test this by measuring the amount of people who enter the building using ID cards, and the timestamp.

### Other Measured Variables
Let's measure things like weather, parking availablity, gym postings, and let's keep the advertisements for the gym constant as not to inflate the amount of people coming to the gym. Measure monthly revenue / profit.


## Problem #3

*Will a new homepage improve my online exotic pet rental business?*

For this we can easily perform an A/B test. should be fun.


### Versions

For control, we can choose the current homepage. For test, we choose the newly designed website. We may want more website versions but the old and new version is fine for now.

### Sample

Our sample should be **dead simple**. Just take ~50% of our visitors and show them the old site, and the other 50% the new site. We can use a session cookie or their IP and device ID to make sure they don't see both versions, keeping the sample separate.

### Hypothesis

My hypothesis is that we will get more people applying for exotic pet rentals if we create the new website.

### Outcomes

The outcome we're looking for is more signups for exotic pet rentals. We can measure this by tracking the amount of users who sign up after being shown the new vs. old website version in the testing time period, say over a month or quarter. Tracking traffic flow before signups. On-site page analytics.

### Other Measured Variables

We will measure where the requests were coming from, maybe an ad was issued that may skew the results, causing people to sign up not because of the website, but because of the ad. We can also measure how long users stayed on the website, regardless if they actually signed up for a "pet lion". That will still be useful in figuring out the effectiveness of the website in general, other than increased customer base.

## Problem #4

*If I put 'please read' in the email subject will more people read my emails?*

For this one, we only have one good option: Clickthrough rate.

### Versions

For the sake of this example, let's assume we have a list of emails on a newsletter. We can send emails to all emails on the newsletter, sending ~50% with the 'please read' in the subject line.

### Sample

The sample will be of people familiar to our product, service, or whatnot, as they're on the newsletter. The population will be split into 2 samples, each of equal size, at random. Since we just have emails, we won't have info like age, gender, previous interest, etc.

### Hypothesis

My hypothesis is that 'please read' will just piss some people off, and so they just won't read and won't click through the email. That's what I'd do. Like piss off.

### Outcomes

The outcome we're looking for is clickthrough rate. Each email can have every link on it tagged with an identifier to inform us of which email it was sent from. I.e. was it in the email with the `'please read'` subject line? We'll measure that over the course of the test period (let's say a month) to determine weather the email had any effect.

### Other measured variables

Another variable we can measure is the amount of times a device ID and IP clicks the email (this can help with detecting outliers that would skew our data). We can also measure the time taken to open a link. This can be used to figure out the delay. Maybe people click faster when the `'please read'` subject line is included

