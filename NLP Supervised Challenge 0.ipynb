{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/ryan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/ryan/anaconda3/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# define a new bag of words approach that should get us some additional information\n",
    "# we want to look at number of words in sentence, how many times puncutation is used, how many nouns, verbs, adj, etc. length of previous sentence vs. length of next sentence\n",
    "def bow_feature_generator(sentences, common_words, include_extra_counts=True):\n",
    "    # we want to go through each sentence and count the number of occurances of verbs, nouns, adj, etc.\n",
    "    # we also want to count how many words are in each text\n",
    "    rows = []\n",
    "    for index, row in enumerate(sentences.itertuples()):\n",
    "        \n",
    "        sentence = row.text_sentence\n",
    "        source = row.text_source\n",
    "        \n",
    "        info_row = collections.defaultdict(int)\n",
    "        \n",
    "        for token in sentence:\n",
    "            if include_extra_counts:\n",
    "                part_of_speech = token.pos_\n",
    "\n",
    "                if part_of_speech == \"VERB\":\n",
    "                    # if it is a verb, add 1 for the sentence\n",
    "                    info_row[\"n_verbs\"] += 1\n",
    "                elif part_of_speech == \"NOUN\":\n",
    "                    # if it is a noun, add 1 for the noun to the sentence\n",
    "                    info_row[\"n_nouns\"] += 1\n",
    "\n",
    "                elif part_of_speech == \"ADJ\":\n",
    "                    # if it is an adjective, add 1 for the adjective count for this sentence\n",
    "                    info_row[\"n_adjectives\"] += 1\n",
    "\n",
    "                info_row[\"n_words\"] += 1\n",
    "                \n",
    "                \n",
    "                if token.is_punct:\n",
    "                    info_row[\"n_puncutated_words\"] += 1\n",
    "                    \n",
    "                if token.is_right_punct:\n",
    "                    info_row[\"n_right_puncutated_words\"] += 1\n",
    "                \n",
    "                if token.is_left_punct:\n",
    "                    info_row[\"n_left_puncutated_words\"] += 1\n",
    "                \n",
    "                # named entities\n",
    "                if token.ent_type_ == \"PERSON\":\n",
    "                    info_row[\"n_people_mentioned\"] += 1\n",
    "                \n",
    "                if token.ent_type_ == \"LOC\":\n",
    "                    info_row[\"n_locations_metioned\"] += 1\n",
    "                \n",
    "                if token.ent_type_ == \"PRODUCT\":\n",
    "                    info_row[\"n_products_mentioned\"] += 1\n",
    "            \n",
    "            if not token.is_punct and not token.is_stop and token.lemma_ in common_words:\n",
    "                info_row[token.lemma_] += 1\n",
    "\n",
    "        \n",
    "        info_row = dict(info_row)\n",
    "        info_row[\"text_sentence\"] = sentence\n",
    "        info_row[\"text_source\"] = source\n",
    "        rows.append(info_row)\n",
    "    \n",
    "    if include_extra_counts:\n",
    "        extra_columns = [\"n_verbs\", \"n_nouns\",\n",
    "                         \"n_adjectives\", \"n_words\",\n",
    "                         \"n_puncutated_words\", \"n_right_puncutated_words\",\n",
    "                         \"n_left_puncutated_words\", \"n_people_mentioned\",\n",
    "                         \"n_locations_metioned\", \"n_products_mentioned\"\n",
    "                        ]\n",
    "    else:\n",
    "        extra_columns = []\n",
    "    \n",
    "    df = pd.DataFrame(data=rows, columns=[\"text_sentence\"] + extra_columns + list(common_words) + [\"text_source\"])\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_sentence text_source\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll\n",
       "3                                      (Oh, dear, !)     Carroll\n",
       "4                                      (Oh, dear, !)     Carroll"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences[\"text_sentence\"] = sentences[0]\n",
    "sentences[\"text_source\"] = sentences[1]\n",
    "sentences = sentences[[\"text_sentence\", \"text_source\"]]\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(models, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    for model, grid_parameters in models.items():\n",
    "        print(\"Running for {}\".format(str(model.__name__)))\n",
    "        print(\"--------\")\n",
    "        grid = GridSearchCV(model(), grid_parameters, refit=True, verbose=0)\n",
    "        \n",
    "        # fit\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        # print the score, and the best parameters\n",
    "        print(\"Best Score: {}\".format(grid.best_score_))\n",
    "        print(\"Train Score: {}\".format(grid.score(X_train, y_train)))\n",
    "        print(\"Test Score: {}\".format(grid.score(X_test, y_test)))\n",
    "        print(\"Best Params: {}\".format(grid.best_params_))\n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some grid searching tools for Logistic Regression, Gradient Boosting, Random Forrest, and Support Vector Classifier\n",
    "\n",
    "# logistic regression parameters\n",
    "lr_params = {\"C\": [0.1, 1.0, 10., 100.], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# gradient boosting possible parameters\n",
    "gb_params = {\"loss\": [\"deviance\", \"exponential\"], \"n_estimators\": [5, 10, 50, 100, 1000], \"subsample\": [0.1, 0.3, 0.6, 0.9, 1]}\n",
    "\n",
    "# random forrest parameters\n",
    "rdf_params = {\"n_estimators\": [1, 5, 10, 50, 100, 1000], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# support vector classifier parameters\n",
    "svc_params = grid_parameters = {'C': [1, 10, 100, 1000], 'gamma': [1, 0.1, 0.001, 0.0001, \"scale\", \"auto\"], 'kernel': ['linear','rbf', 'sigmoid']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    LogisticRegression: lr_params,\n",
    "    ensemble.GradientBoostingClassifier: gb_params,\n",
    "    ensemble.RandomForestClassifier: rdf_params,\n",
    "    SVC: svc_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = bow_feature_generator(sentences, common_words, include_extra_counts=False)\n",
    "data2 = bow_feature_generator(sentences, common_words, include_extra_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>n_verbs</th>\n",
       "      <th>n_nouns</th>\n",
       "      <th>n_adjectives</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_puncutated_words</th>\n",
       "      <th>n_right_puncutated_words</th>\n",
       "      <th>n_left_puncutated_words</th>\n",
       "      <th>n_people_mentioned</th>\n",
       "      <th>n_locations_metioned</th>\n",
       "      <th>...</th>\n",
       "      <th>attentive</th>\n",
       "      <th>natural</th>\n",
       "      <th>man</th>\n",
       "      <th>mile</th>\n",
       "      <th>Penelope</th>\n",
       "      <th>farmer</th>\n",
       "      <th>clever</th>\n",
       "      <th>Australia</th>\n",
       "      <th>make</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>63</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_sentence  n_verbs  n_nouns  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     13.0     11.0   \n",
       "1  (So, she, was, considering, in, her, own, mind...     11.0      8.0   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...      5.0      2.0   \n",
       "3                                      (Oh, dear, !)      0.0      0.0   \n",
       "4                                      (Oh, dear, !)      0.0      0.0   \n",
       "\n",
       "   n_adjectives  n_words  n_puncutated_words  n_right_puncutated_words  \\\n",
       "0           1.0       67                10.0                       4.0   \n",
       "1           6.0       63                 7.0                       1.0   \n",
       "2           1.0       30                 3.0                       1.0   \n",
       "3           1.0        3                 1.0                       0.0   \n",
       "4           1.0        3                 1.0                       0.0   \n",
       "\n",
       "   n_left_puncutated_words  n_people_mentioned  n_locations_metioned  \\\n",
       "0                      4.0                 1.0                   0.0   \n",
       "1                      1.0                 0.0                   0.0   \n",
       "2                      1.0                 2.0                   0.0   \n",
       "3                      0.0                 0.0                   0.0   \n",
       "4                      0.0                 0.0                   0.0   \n",
       "\n",
       "      ...       attentive  natural  man  mile  Penelope  farmer  clever  \\\n",
       "0     ...             0.0      0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1     ...             0.0      0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "2     ...             0.0      0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "3     ...             0.0      0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "4     ...             0.0      0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "\n",
       "   Australia  make  text_source  \n",
       "0        0.0   0.0      Carroll  \n",
       "1        0.0   1.0      Carroll  \n",
       "2        0.0   0.0      Carroll  \n",
       "3        0.0   0.0      Carroll  \n",
       "4        0.0   0.0      Carroll  \n",
       "\n",
       "[5 rows x 1624 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = data1.drop(columns=[\"text_source\", \"text_sentence\"])\n",
    "target1 = data1[\"text_source\"]\n",
    "\n",
    "features2 = data2.drop(columns=[\"text_source\", \"text_sentence\"])\n",
    "target2 = data2[\"text_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(features1, target1, test_size=0.2, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2, target2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for LogisticRegression\n",
      "--------\n",
      "Best Score: 0.8394366197183099\n",
      "Train Score: 0.9746478873239437\n",
      "Test Score: 0.8539325842696629\n",
      "Best Params: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Running for GradientBoostingClassifier\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.847887323943662\n",
      "Train Score: 0.9802816901408451\n",
      "Test Score: 0.8314606741573034\n",
      "Best Params: {'loss': 'deviance', 'n_estimators': 100, 'subsample': 0.3}\n",
      "\n",
      "Running for RandomForestClassifier\n",
      "--------\n",
      "Best Score: 0.847887323943662\n",
      "Train Score: 1.0\n",
      "Test Score: 0.8426966292134831\n",
      "Best Params: {'criterion': 'gini', 'n_estimators': 50}\n",
      "\n",
      "Running for SVC\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8535211267605634\n",
      "Train Score: 0.9633802816901409\n",
      "Test Score: 0.8651685393258427\n",
      "Best Params: {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_best_model(models_dict, X_train2, X_test2, y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
