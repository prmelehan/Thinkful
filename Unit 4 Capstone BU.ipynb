{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora Clustering\n",
    "\n",
    "We're trying to import text files from 7-8 different authors, each with roughly 10 texts, and we want to merge the corpuses to classify who is writing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_document(document):\n",
    "    document = document.replace(\"\\t\", \" \")\n",
    "    document = document.replace(\"\\n\", \".\")\n",
    "    document = document.replace(\"\\r\", \".\")\n",
    "    document = document.replace(\"\\r\\n\", \".\")\n",
    "    document = document.replace(\"\\n\\r\", \".\")\n",
    "    document = document.replace(\"...\", \".\")\n",
    "    document = document.replace(\"..\", \". \")\n",
    "    \n",
    "    return document.split(\".\") # return all of the sentences in the corpus, each of which is a \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = {\n",
    "    \"Abernathy\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Abernathy/\",\n",
    "    \"Berk\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Berk/\",\n",
    "    \"Castro\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Castro/\",\n",
    "    \"Fletcher\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Fletcher/\",\n",
    "    \"Kauffman\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Kauffman/\",\n",
    "    \"Rybczynski\": cwd + \"/data/AmericanNationCorpus/data/written_2/non-fiction/OUP/Rybczynski/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, directory in directories.items():\n",
    "    os.chdir(directory)\n",
    "    # use glob to find all text files, and load them in\n",
    "    text_paths = glob.iglob(\"*.txt\")\n",
    "    \n",
    "    for text_path in text_paths:\n",
    "        \n",
    "        document = open(text_path).read()\n",
    "        clean_document = cleanup_document(document)\n",
    "        \n",
    "        corpora[name].extend(clean_document)\n",
    "        \n",
    "# go back to the original directory\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: TF-IDF\n",
    "The next sections will create the TF-IDF matrix as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our items into a dataframe\n",
    "rows = []\n",
    "for name, sentences in corpora.items():\n",
    "    for sentence in sentences:\n",
    "        rows.append({\"source\": name, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "sentences_df = pd.DataFrame(rows, columns=[\"sentence\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer sentences\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "# Applying the vectorizer\n",
    "tfidf = vectorizer.fit_transform(sentences_df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17164"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas df from this data, using the feature names\n",
    "df = pd.DataFrame(tfidf.todense(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CLASS_LABEL\"] = sentences_df[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample to 50 features for computational efficiency\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "downsampled_features = PCA(n_components=50).fit_transform(df.drop(columns=[\"CLASS_LABEL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = pd.DataFrame(downsampled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df[\"CLASS_LABEL\"] = df[\"CLASS_LABEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Time!\n",
    "\n",
    "In the cells below we will use the following Clustering techniques to and compare their ARI scores to each other\n",
    "\n",
    "- KMeans\n",
    "- MeanShift\n",
    "- SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = downsampled_df.drop(columns=[\"CLASS_LABEL\"])\n",
    "target = downsampled_df[\"CLASS_LABEL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_clusterer = KMeans(n_clusters=len(set(downsampled_df[\"CLASS_LABEL\"])), n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictive data\n",
    "k_means_data = k_means_clusterer.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score (K-Means): 0.0599961656125327\n"
     ]
    }
   ],
   "source": [
    "k_means_score = adjusted_rand_score(y_train, k_means_data)\n",
    "print(\"ARI Score (K-Means): {}\".format(k_means_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import estimate_bandwidth\n",
    "\n",
    "bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "\n",
    "shift_clusterer = MeanShift(bandwidth=bandwidth, cluster_all=False, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_data = shift_clusterer.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0           0\n",
      "CLASS_LABEL      \n",
      "Abernathy    2247\n",
      "Berk         2195\n",
      "Castro       2111\n",
      "Fletcher     1820\n",
      "Kauffman     3695\n",
      "Rybczynski    805\n"
     ]
    }
   ],
   "source": [
    "# plt.scatter(X_train.values[:, 0], X_train.values[:, 1], c=y_train)\n",
    "# plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(y_train, shift_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score (Mean Shift): 0.0\n"
     ]
    }
   ],
   "source": [
    "shift_score = adjusted_rand_score(y_train, shift_data)\n",
    "print(\"ARI Score (Mean Shift): {}\".format(shift_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_clusterer = SpectralClustering(len(set(downsampled_df[\"CLASS_LABEL\"])), n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_data = spectral_clusterer.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score (Spectral Clustering): 0.08584146214367562\n"
     ]
    }
   ],
   "source": [
    "spectral_score = adjusted_rand_score(y_train, spectral_data)\n",
    "print(\"ARI Score (Spectral Clustering): {}\".format(spectral_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait a sec, this is hilariously random\n",
    "\n",
    "Maybe let's try a different technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "In the next few cells we'll try to build a supervised model that performs using features that we select using an unsupervised technique (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7960848287112561"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
